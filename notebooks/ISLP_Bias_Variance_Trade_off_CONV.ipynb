{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qzh0004/NSF-IUSE/blob/main/notebooks/ISLP_Bias_Variance_Trade_off_CONV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Bias-Variance Trade-Off\n",
        "================\n",
        "\n",
        "The U-shape observed in test MSE is the result of two competing properties of ML methods.\n",
        "\n",
        "The expected test MSE, for a given value $x_0$, can always be decomposed into the sum of three fundamental quantities: the variance of $\\hat{f}(x_0)$, the squared bias of $\\hat{f}(x_0)$ and the variance of the error terms $\\epsilon$.\n",
        "\n",
        "$\\mathbb{E}(y_0-\\hat{f}(x_0))^2=Var(\\hat{f}(x_0)+[Bias(\\hat{f}(x_0))]^2+Var(\\epsilon)$\n",
        "\n",
        "where\n",
        "\n",
        "$Bias(\\hat{f}(x_0))=f(x_0)-\\mathbb{E}(\\hat{f}(x_0))$\n",
        "\n",
        "The purpose of this notebook is to illustrate the bias-variance trade-off using a numerical example.\n",
        "\n",
        "Training data generation\n",
        "------------------------\n",
        "\n",
        "First we will write a function to generate a random sample. The data generation model is the following:\n",
        "\n",
        "$y = 2sin(1.5x) + \\epsilon$\n",
        "\n",
        "with $\\epsilon\\sim\\mathcal{N}(0,0.5)$"
      ],
      "metadata": {
        "id": "vE5Api1-eOj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we define the underlying functions: $f=2sin(1.5x)$, which is not known for real world problems, and the observed response $y=f+\\epsilon$."
      ],
      "metadata": {
        "id": "VNwKZfK8kkBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "global sigma\n",
        "sigma=0.5 # standard deviation of the error term (variance=sigma**2)\n",
        "np.random.seed(seed=0) # define random seed so the results are repeatable.\n",
        "def f(x):\n",
        "    '''\n",
        "    Returns a sample with instances/observations without noise (true but unknown value).\n",
        "    '''\n",
        "    yt = 2 * np.sin(x * 1.5)\n",
        "    return yt\n",
        "\n",
        "def y_m(x):\n",
        "    '''\n",
        "    Retrun a sample with instances/observations with noise/error (measured value).\n",
        "    '''\n",
        "    global sigma\n",
        "    y = 2 * np.sin(x * 1.5) + np.random.normal(0, sigma, x.size)\n",
        "    return y\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "7jOPmaa8eOj8"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The function ($f(x) = 2 \\sin(1.5x)$) represents the true (but unknown) relationship between the input ($x$) and output ($y$). However, in real-world scenarios we typically do not know this true function (that's why we try to approximate or learn it from data).\n",
        "\n",
        "2. The variable $y$ in the code is a noisy version of $f$. Specifically, $y$ adds a random noise/error term $\\epsilon$ to the true function $f$. This noise represents measurement error, as well as other factors (e.g., instrument errors, missing variables, randomness in the process, etc.)."
      ],
      "metadata": {
        "id": "gN5UA_R9geGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, $f$ is usually unknown Besides measurement noise (i.e., unmeasurable variation), there are other factors that could contribute to $\\epsilon$. For example, some variables that are useful in predicting $y$ may have not been included in $X$ because of our limited knowledge of the system or process for example. Since we don't measure them, $f$ cannot use them for its prediction.\n"
      ],
      "metadata": {
        "id": "hweo7mdIIfzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we plot $f$ (underlying unknown function) as a line/curve, and a sample of $y$ (sample size 50, *i.e.*, 50 instances/observations) as dots."
      ],
      "metadata": {
        "id": "muyfRZl5k47U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "n_obs = 50 # number of observations in each sample.\n",
        "xm = np.linspace(0, 4.5, n_obs)\n",
        "xt=np.linspace(0, 4.5, 100)\n",
        "yt = f(xt)\n",
        "plt.plot(xt, yt,'r')\n",
        "ym = y_m(xm)\n",
        "plt.plot(xm, ym, 'k.')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0AowcnoZlJR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "n_obs specifies the number of observations. Right now it's 50. You can change it to test out different values (e.g., 500 or 5000) to see how that change the plot (both the red line and the black points).\n",
        "\n",
        "If n_obs is too small (e.g., 5), it would be difficult for us to imagine the true function form without knowing it."
      ],
      "metadata": {
        "id": "3ieG-GUF7IoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " In practice, the noise or error magnitude can vary from application to application. It even depends on the units of the output. For example, $mm$ vs. $m$ used in length measurement. This magnitude is controlled in this simulated example? Can you simulate a scenario where the noise/error is very small?Change sigma=0.5 in the first code cell to sigma=0.05. Then click on Runtime menu and select Rull all."
      ],
      "metadata": {
        "id": "lKseRIsdOqBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model fitting\n",
        "=============\n",
        "\n",
        "We will use least square regression (LSR) to fit a polynomial to the data. Actually, we will use multivariate linear regression, over a dataset built in the following way:\n",
        "\n",
        "For each sample $x_{i}$ we build a vector $(1 , x_{i} , x_{i}^{2} , \\dots , x_{i}^{n})$  and we use LSR to fit a function $g:\\mathbb{R}^{n+1}\\rightarrow\\mathbb{R}$ to the training data. If you don't understand the following section, don't worry. It will not affect your progress of the sections followed."
      ],
      "metadata": {
        "id": "0GBpoQLVeOj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One example is shown below where we fit the sample that we obtained earlier using an 8-th order polynomial. Still, the sample instances/points are shown as dots, the true model as red line/curve and the fitted/estimated model as blue line/curve."
      ],
      "metadata": {
        "id": "1crfb5Ren7R9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from os import XATTR_SIZE_MAX\n",
        "from sklearn.linear_model import LinearRegression  # liner regression model\n",
        "from sklearn.preprocessing import PolynomialFeatures # polynommial features(extended features)\n",
        "\n",
        "d=5; #degree of polynomial\n",
        "poly_features = PolynomialFeatures(degree=d) # decide the maximal degree of the polynomial feature\n",
        "xx=xm.reshape(-1, 1)\n",
        "x_poly = poly_features.fit_transform(xx) # convert the original feature to polynomial feature\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(x_poly,ym)\n",
        "yp = lin_reg.predict(x_poly)\n",
        "plt.plot(xm, ym, 'k.')\n",
        "plt.plot(xt, yt,'r')\n",
        "plt.plot(xm, yp,'b')\n"
      ],
      "metadata": {
        "id": "LQOvdikioaUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though polynomials might not be the most intuitive choice for a periodic function, for this particular range of $x$, a lot of polynmials provide good approximation."
      ],
      "metadata": {
        "id": "8acBJxZt9Vzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We often pick certain models in practice due to convenience, simplicity of the model, familiarity with the methods, or the general approximation capability of the model (e.g., polynomials), even if the functional form is different. Remember that for most real applications, we don't have the knowledge of the true model, although sometimes we do know the form/structure of the model, e.g., sinusoidal, without knowing the specific parameter values."
      ],
      "metadata": {
        "id": "CeBX6Sf7EMib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without thinking about bias-variance trade-off, which we will get to in a minute, we can observe that low degree (e.g., $d=1$ or $d=2$) fails to capture the sine wave shape (a.k.a., underfitting). while high degree (e.g., $d=16$) fits to the noise (a.k.a., overfitting)."
      ],
      "metadata": {
        "id": "LPg17SRiBxCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we repeat the experiment (with new random noise) for the same polynomial degree, we would not get exactly the same polynomial fit. This is clear from the next section where we repeat the model fitting process for different samples, which lead us to the concept of $Var(\\hat{f}(x))$."
      ],
      "metadata": {
        "id": "curPCI-KJNQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Bias and Variance\n",
        "---------------\n",
        "\n",
        "The following code generates a set of `n_sam` samples, allowing us to estimate $\\hat{f}$ `n_sam` times. Those models are plotted as black lines.\n"
      ],
      "metadata": {
        "id": "QO7UQ-OneOj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = np.array([4.6]) # the test sample of x=4.6\n",
        "xx_new=np.append(xx,x0)\n",
        "xx_new=xx_new.reshape(-1, 1)\n",
        "x0_poly = poly_features.fit_transform(xx_new) # convert the original feature to polynomial feature\n",
        "degree = 3 # model polynomial order. you can change it to see how it affects the model\n",
        "n_obs = 50 # number of observations in each sample.\n",
        "n_sam= 20 # number of samples\n",
        "yt0=np.zeros(n_sam) # true y, which is not known in the real world\n",
        "ym0=np.zeros(n_sam) # measured y with error\n",
        "yp0=np.zeros(n_sam) # predicted y\n",
        "for i in range(n_sam):\n",
        "    y = y_m(xm)\n",
        "    model = LinearRegression()\n",
        "    model.fit(x_poly,y)\n",
        "    yp = model.predict(x_poly) # make predictions using trained Linear Regression model\n",
        "    tmp=model.predict(x0_poly)\n",
        "    yp0[i]=tmp[-1]\n",
        "    ym0[i]=y_m(x0)\n",
        "    yt0[i]=f(x0)\n",
        "    plt.plot(xm, yp, 'k-')\n",
        "plt.plot(xt, yt,'r')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KNgeM0MXlvPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea is to show how different random draws of noise affect the learned model, which highlights model variance when trained with different training sets. In real applications, we don't know the noise source(s) and how their realization, hence there's always uncertainty or variance in learned/trained models.\n",
        "\n"
      ],
      "metadata": {
        "id": "Iru2Cy_bxVFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though $f(x)$ is fixed, the random noise in each sample leads the model to fit slightly differently, causing variation in predictions. Seeing multiple samples reveals the variability of the model, which is invisible when only looking at a single dataset."
      ],
      "metadata": {
        "id": "BCkQ8sFX22_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each sample, we get a different predicted value at $x=4.6$ (the unseen test sample), illustrated as the last points of all curves. (Note that this is also true for the training samples.) The spread (variance) of the 20 predictions at $x=4.6$ inform us about the reliability or stability of the model. A large spread indicates high variance; a small spread indicates more stable predictions."
      ],
      "metadata": {
        "id": "DIIJxLaDy3t-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculation and visualization of bias and variance\n",
        "Now we have predictions from 10 models trained by 10 samples. We can calucate the bias and variance and visualize them on graphs."
      ],
      "metadata": {
        "id": "jk9tvY-T43q3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "# Calculate stats\n",
        "mean1, var1 = np.mean(yt0), np.var(yt0)\n",
        "mean2, var2 = np.mean(yp0), np.var(yp0)\n",
        "bin = [1.16-0.05, 1.16+0.05]  # Just 0.01 wide\n",
        "# Plot overlapping histograms with KDE\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(yt0, bins=bin, kde=False, color='blue', label='True value', alpha=0.5)\n",
        "sns.histplot(yp0, color='red', label='Predicted value', alpha=0.5, kde=True)\n",
        "\n",
        "# Add vertical lines for the means\n",
        "plt.axvline(mean1, color='blue', linestyle='--')\n",
        "plt.axvline(mean2, color='red', linestyle='--')\n",
        "\n",
        "# Annotate mean and variance near each vertical line\n",
        "# (You can adjust the x and y positions to suit your data)\n",
        "ymax = plt.ylim()[1]  # Get top limit of y-axis for positioning text\n",
        "plt.text(mean1, 0.9*ymax, f\"mean={mean1:.2f}\\nvar={var1:.2f}\", color='blue')\n",
        "plt.text(mean2, 0.7*ymax, f\"mean={mean2:.2f}\\nvar={var2:.2f}\", color='red')\n",
        "\n",
        "plt.title(\"Overlapping Histograms with Mean & Variance\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1WjNG7S0vaEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plot both the true values (yt0) and predicted values (yp0) on the same histogram because we want to see how closely the distribution of predictions matches the (unknown in practice) true value distribution. Since we repeated the experiment with different random noise, we get multiple predicted values vs. a single “true” values."
      ],
      "metadata": {
        "id": "HYmYhgMvLG8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The difference in location between the two histogram peaks (or the difference in mean) represent the bias."
      ],
      "metadata": {
        "id": "nQH956SzOngJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "yt0 is a single constant repeated 20 times. Because the true function is deterministic at a single $x$, it has no variation."
      ],
      "metadata": {
        "id": "BfaQQnoqPUo7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typically, the true function $f(x)$ does not vary over multiple observations at the exact same $x$ —the underlying relationship is assumed deterministic at a specific $x$. Any variation in actual measurements (i.e., $y$) at the same $x$ would come from noise, not from $f$ itself."
      ],
      "metadata": {
        "id": "jfaY7DtlRsNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bias\n",
        "$Bias=|f(x_0)-\\mathbb{E}[\\hat{f}(x_0)]|$, where $f(x_0)$ is the true value and $\\mathbb{E}[\\hat{f}(x_0)]$ can be estimated as the mean or average of $\\hat{f}(x_0)$."
      ],
      "metadata": {
        "id": "d_YFHaPT1hYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate bias (absolute difference of means)\n",
        "Bias=abs(mean1-mean2)\n",
        "print(\"Bias =\", Bias)"
      ],
      "metadata": {
        "id": "RwX_ct3I0vsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bias is not the average difference between predictions and their own mean, but rather the average difference between predictions and the true value. This is because the bias is about the systematic difference between the mean prediction and the true value.\n"
      ],
      "metadata": {
        "id": "MfvenUgDX-Ak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variance\n"
      ],
      "metadata": {
        "id": "vnl-SCI42-IR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate variance of the predictions around their own mean\n",
        "Var=np.var(yp0-mean2) # Variance\n",
        "print(\"Variance =\", Var)"
      ],
      "metadata": {
        "id": "nPK5BdYm3dRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We measure the variance around the mean prediction rather than around the true value. Variance in the bias-variance decomposition focuses on how $\\hat{f}$ fluctuates around its own mean (i.e., the expected prediction) across repeated samples. A high variance means small changes in training data can lead to large changes in predictions, making the model “unstable.”"
      ],
      "metadata": {
        "id": "nWRUInqpX8T9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that without subtracting mean from $\\hat{f}(x_0)$ won't affect the variance calculation."
      ],
      "metadata": {
        "id": "LNKgH-EL3rDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Var=np.var(yp0) # Variance\n",
        "print(\"Variance =\", Var)"
      ],
      "metadata": {
        "id": "jSl94iYE365B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Total error and mean squared error (MSE)\n",
        "$\\mathbb{E}[(y_0)-\\hat{f}(x_0))^2]=Var(\\hat{f}(x_0))+(Bias(\\hat{f}(x_0)))^2+\\mathbb{E}[\\epsilon^2]$\n",
        "\n",
        "Note that $\\mathbb{E}[\\epsilon^2]=Var(\\epsilon)=\\sigma^2$"
      ],
      "metadata": {
        "id": "1bS07moG4SQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Total error (theoretical formula for error decomposition: error = bias^2 + variance + noise_variance)\n",
        "TotErr=Var+Bias**2+sigma**2\n",
        "print(\"Total Error =\", TotErr)\n",
        "# MSE at x=4.6: mean of (prediction - true_value)^2 over all samples\n",
        "MSE = np.mean((yp0 - yt0)**2)\n",
        "print(\"MSE =\", MSE)"
      ],
      "metadata": {
        "id": "cZsWpSJy5a3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at $Var+Bias^2+\\sigma^2$, each of these terms contribute to total error. Each term has a distinct role:\n",
        "1. $Bias^2$ is the squared distance between the average prediction and the true value (systematic error).\n",
        "2. $Var$ reflects how sensitive the model is to the particular training set (random fluctuations in fit).\n",
        "3. $\\sigma^2$ is the irreducible noise in the data."
      ],
      "metadata": {
        "id": "1AVtoN6JXZhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Also note that $Var(\\epsilon)$ is unknown in reality. But it can be estimated using measured response, i.e., $y$. If we assume $y$ follows (multivariate) Gaussion or Normal distribution, $Var(\\epsilon)=Var(y)$."
      ],
      "metadata": {
        "id": "08D4DdJX7hdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VarEst=np.var(ym0)\n",
        "print(VarEst)"
      ],
      "metadata": {
        "id": "4FwAN1668GEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that because of the small sample size, which is 20 in this case, the estimated variance will not be an accurate estimate of the true value. Again, the true value is only known in simulated cases but unknown in any real applications."
      ],
      "metadata": {
        "id": "D4qMo7Sr8bOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Bias-Variance Trad-Off\n",
        "\n",
        "Above we only calculated bias, variance and MSE for a fixed model flexibility (i.e., the polynomial order). Now we calculate them for a range of model flexibilities (i.e., a range of polynomial orders)."
      ],
      "metadata": {
        "id": "QDWhZ2fD9RYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "n_sam2 = 20   # number of samples\n",
        "n_ord = 11    # maximum polynomial degree to examine\n",
        "\n",
        "# Arrays now of length n_ord\n",
        "bia_all  = np.zeros(n_ord)  # bias^2\n",
        "var_all  = np.zeros(n_ord)  # variance\n",
        "MSE_all1 = np.zeros(n_ord)  # MSE = bias^2 + variance + noise^2\n",
        "MSE_all2 = np.zeros(n_ord)  # MSE from (ym0 - yp0)**2\n",
        "\n",
        "for d in range(1, n_ord + 1):\n",
        "    # Define polynomial features\n",
        "    poly_features = PolynomialFeatures(degree=d)\n",
        "    x_poly = poly_features.fit_transform(xx)\n",
        "    x0_poly = poly_features.fit_transform(xx_new)\n",
        "\n",
        "    # Arrays to store per-sample values\n",
        "    yt0 = np.zeros(n_sam2)  # true y\n",
        "    ym0 = np.zeros(n_sam2)  # measured y with noise\n",
        "    yp0 = np.zeros(n_sam2)  # predicted y\n",
        "\n",
        "    for i in range(n_sam2):\n",
        "        ym = y_m(xm)\n",
        "        model = LinearRegression()\n",
        "        model.fit(x_poly, ym)\n",
        "\n",
        "        # Predictions for training inputs and x0\n",
        "        yp = model.predict(x_poly)\n",
        "        tmp = model.predict(x0_poly)\n",
        "        yp0[i] = tmp[-1]\n",
        "\n",
        "        ym0[i] = y_m(x0)\n",
        "        yt0[i] = f(x0)\n",
        "\n",
        "    mean_true = np.mean(yt0)\n",
        "    mean_pred = np.mean(yp0)\n",
        "\n",
        "    # Compute bias^2\n",
        "    bias_sq = (mean_true - mean_pred)**2\n",
        "    bia_all[d-1] = bias_sq\n",
        "\n",
        "    # Compute variance of predictions\n",
        "    var_all[d-1] = np.var(yp0)\n",
        "\n",
        "    # Theoretical MSE\n",
        "    MSE_all1[d-1] = var_all[d-1] + bias_sq + sigma**2\n",
        "\n",
        "    # Empirical MSE\n",
        "    MSE_all2[d-1] = np.mean((ym0 - yp0)**2)\n",
        "\n",
        "# Polynomial orders for the x-axis\n",
        "degrees = range(1, n_ord + 1)\n",
        "\n",
        "# Plot\n",
        "plt.plot(degrees, bia_all, 'ro-', label='bias^2')\n",
        "plt.plot(degrees, var_all, 'bo-', label='variance')\n",
        "plt.plot(degrees, MSE_all1, 'go-', label='MSE1')\n",
        "plt.plot(degrees, MSE_all2, 'yo-', label='MSE2')\n",
        "\n",
        "plt.xlabel('Polynomial Order')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WQZihy8y3G39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the $bias^2$ (bia^2) and $Var$ (variance) curves as we increase the polynomial order from 1 to 11, we can see that bias typically decreases as the polynomial degree increases, because a more flexible model can better fit the underlying sine function. Variance typically increases with increasing polynomial degree, because the model becomes more sensitive to noise in each training sample."
      ],
      "metadata": {
        "id": "FM8sCRtADM6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE1 is theoretical, derived from $Var(\\hat{f})+Bias^2+\\sigma^2$. \\\\\n",
        "MSE2 is empirical, the mean of squared differences between the predicted $\\hat{f}(x_0)$ and the noisy measurement $y(x_0)$. \\\\\n",
        "They both quantify errors, but MSE1 uses $\\sigma^2$, the variance of the error term, which is unknown in real applications. MSE2 uses actual measurements without the knowledge of $\\sigma$. Because of finite samples and randomness, the two curves generally align but may not match exactly. \\\\\n",
        "Due to the reasons discussed above, in real applications, we can only obtain MSE2. However, as also discussed above, MSE2 represents MSE1 well in general."
      ],
      "metadata": {
        "id": "6B3gm-FeHFka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MSE curves often form a U-shape with respect to model complexity: if the model is too simple, bias is large; if too complex, variance is large. The optimal polynomial degree is usually in the middle, balancing bias and variance."
      ],
      "metadata": {
        "id": "dpPpABmiEpVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Low-degree polynomial (e.g., 1 or 2) is more rigid, hence its parameters are less affected by the randomness in training samples. This leads to small variance in models. However, the rigidness does lead to higher bias in general. \\\\\n",
        "In contrast, higher-degree polynomials can adapt more closely to the underlying function, reducing systematic error (bias). However, the high flexibility also means that the model parameters are easier to be affected by (or overreact to) the randomness in the training samples, hence higher variance."
      ],
      "metadata": {
        "id": "qtUPtf1-FwdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practical settings, how do data scientists pick model complexity (e.g., polynomial degree, neural network size) when they can't measure true bias or variance directly? They typically rely on empirical error metrics (like MSE2), cross-validation, or hold-out validation sets. Concepts from the bias-variance trade-off guide them to avoid both underfitting and overfitting."
      ],
      "metadata": {
        "id": "stcbgATVeKnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "This notebook is adapted and improved based on: https://gist.github.com/fabgoos/6788818"
      ],
      "metadata": {
        "id": "wfYas41tgKSc"
      }
    }
  ]
}